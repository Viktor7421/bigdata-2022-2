{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problema: Reducir  el tiempo de busqueda de un Robot a un item en un area que pertenece a una planta industrial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "# Environment\n",
    "# policy\n",
    "# reward\n",
    "# *render,  gym\n",
    "\n",
    "import os\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "    0 1 2 3 4\n",
    "    5 6 7 8 9\n",
    "    ....\n",
    "    .\n",
    "\"\"\"\n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self,num_actions: int):\n",
    "        self.n = num_actions\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n-1)\n",
    "\n",
    "        \"\"\"\n",
    "            0 1 2 3 4\n",
    "            5 6 7 8 9\n",
    "            ...\n",
    "                23 24\n",
    "        \"\"\"\n",
    "\n",
    "class Environment:\n",
    "    # X,Y plane: UP, DOWN, RIGHT, LEFT\n",
    "    seeker, goal = (0,0), (4,4)\n",
    "    info = {'seeker': seeker, 'goal': goal}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(25)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.seeker = (0,0)\n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        return 5*self.seeker[0]+self.seeker[1]\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        return self.seeker == self.goal\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   # move down\n",
    "            self.seeker = (min(self.seeker[0]+1,4), self.seeker[1])\n",
    "        elif action == 1: # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1]-1,0))\n",
    "        elif action == 2: # move up\n",
    "            self.seeker = (max(self.seeker[0]-1,0), self.seeker[1])\n",
    "        elif action == 3: # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1]+1,4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        return self.get_observation(), self.get_reward(), self.is_done(), self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        # os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        clear_output()\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "        \n",
    "import numpy as np\n",
    "class Policy:\n",
    "    def __init__(self, env):\n",
    "        self.state_action_table = [[0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)] # observation_space = 25\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "    def get_action(self, state, explore=True, epsilon=0.1 ):\n",
    "        if explore and (random.uniform(0,1) < epsilon):\n",
    "            return self.action_space.sample()\n",
    "        print(\"state:\", state)\n",
    "        print(\"table state:\", self.state_action_table[state])\n",
    "        print(\"argmax: \", np.argmax(self.state_action_table[state]))\n",
    "        return np.argmax(self.state_action_table[state])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feda6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n",
      "8.0 steps on average  for a total of 10 episodes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class Simulation(object):\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # random_action = env.action_space.sample()\n",
    "            action = policy.get_action(state, explore, epsilon)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            experiences.append([state, action, reward, next_state])    \n",
    "            state = next_state\n",
    "            if render:\n",
    "                time.sleep(0.5)\n",
    "                self.env.render()\n",
    "        return experiences\n",
    "\n",
    "def train_policy(env, num_episodes=10000):\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)\n",
    "        update_policy(policy, experiences)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def update_policy(policy, experiences):\n",
    "    alpha=0.1\n",
    "    discount_factor=0.9\n",
    "    for state, action, reward, next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = value*(1-alpha) + alpha*(reward+discount_factor*next_max)\n",
    "        policy.state_action_table[state][action] = new_value\n",
    "\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)\n",
    "        steps += len(experiences)\n",
    "    \n",
    "    print(f\"{steps/num_episodes} steps on average \"\n",
    "            f\" for a total of {num_episodes} episodes\")\n",
    "\n",
    "\n",
    "env = Environment()\n",
    "opt_policy = train_policy(env)\n",
    "\n",
    "evaluate_policy(env, opt_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9da0908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bigdata-2022-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee0e340b373adaa70299d42cd1cb59b0a3467f40584d69eef1fc62bec46809f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
