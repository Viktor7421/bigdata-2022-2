{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tag::load_cifar[]\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "def load_cifar(train: bool):\n",
    "    transform = transforms.Compose([  # <1>\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    def loader():\n",
    "        return datasets.CIFAR10(  # <2>\n",
    "            root=\"./data\",\n",
    "            download=True,\n",
    "            train=train,  # <3>\n",
    "            transform=transform)\n",
    "\n",
    "    return loader\n",
    "# end::load_cifar[]\n",
    "\n",
    "# tag::std[]\n",
    "from ray.data import read_datasource, datasource\n",
    "\n",
    "\n",
    "source = datasource.SimpleTorchDatasource()  # <1>\n",
    "train_dataset = read_datasource(source, dataset_factory=load_cifar(train=True))  # <2>\n",
    "test_dataset = read_datasource(source, dataset_factory=load_cifar(train=False))\n",
    "# end::std[]\n",
    "\n",
    "\n",
    "# tag::batch_conversion[]\n",
    "import pandas as pd\n",
    "from ray.data.extensions import TensorArray\n",
    "\n",
    "\n",
    "def convert_to_pandas(batch):\n",
    "    return pd.DataFrame({\n",
    "        \"image\": TensorArray([image.numpy() for image, _ in batch]),  # <1>\n",
    "        \"label\": [label for _, label in batch]  # <2>\n",
    "    })\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map_batches(convert_to_pandas)  # <3>\n",
    "test_dataset = test_dataset.map_batches(convert_to_pandas)\n",
    "# end::batch_conversion[]\n",
    "\n",
    "\n",
    "# tag::torch_model[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# end::torch_model[]\n",
    "\n",
    "# tag::torch_training_loop[]\n",
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "\n",
    "\n",
    "def train_loop(config):\n",
    "    model = train.torch.prepare_model(Net())  # <1>\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_batches = session.get_dataset_shard(\"train\").iter_torch_batches(  # <2>\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_batches):\n",
    "            inputs, labels = data[\"image\"], data[\"label\"]  # <3>\n",
    "\n",
    "            optimizer.zero_grad()  # <4>\n",
    "            forward_outputs = model(inputs)\n",
    "            loss = loss_fct(forward_outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()  # <5>\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"[{epoch + 1}, {i + 1:4d}] loss: {running_loss / 1000:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        session.report(  # <6>\n",
    "            dict(running_loss=running_loss),\n",
    "            checkpoint=Checkpoint.from_dict(dict(model=model.module.state_dict())),\n",
    "        )\n",
    "# end::torch_training_loop[]\n",
    "\n",
    "\n",
    "# tag::torch_trainer[]\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "from ray.air.callbacks.mlflow import MLflowLoggerCallback\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    train_loop_config={\"batch_size\": 10, \"epochs\": 5},\n",
    "    datasets={\"train\": train_dataset},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    run_config=RunConfig(callbacks=[\n",
    "        MLflowLoggerCallback(experiment_name=\"torch_trainer\")\n",
    "    ])\n",
    "\n",
    ")\n",
    "result = trainer.fit()\n",
    "# end::torch_trainer[]\n",
    "\n",
    "# tag::store_checkpoint[]\n",
    "CHECKPOINT_PATH = \"torch_checkpoint\"\n",
    "result.checkpoint.to_directory(CHECKPOINT_PATH)\n",
    "# end::store_checkpoint[]\n",
    "\n",
    "import sys\n",
    "sys.exit(\"End of executable script. Stopping smoke tests here.\")\n",
    "\n",
    "# tag::custom_data[]\n",
    "from ray.data import read_datasource, datasource\n",
    "\n",
    "\n",
    "class SnowflakeDatasource(datasource.Datasource):\n",
    "    pass\n",
    "\n",
    "\n",
    "dataset = read_datasource(SnowflakeDatasource(), ...)\n",
    "# end::custom_data[]\n",
    "\n",
    "\n",
    "# tag::custom_trainer[]\n",
    "from ray.train.data_parallel_trainer import DataParallelTrainer\n",
    "\n",
    "\n",
    "class JaxTrainer(DataParallelTrainer):\n",
    "    pass\n",
    "\n",
    "\n",
    "trainer = JaxTrainer(\n",
    "    ...,\n",
    "    scaling_config=ScalingConfig(...),\n",
    "    datasets=dict(train=dataset),\n",
    ")\n",
    "# end::custom_trainer[]\n",
    "\n",
    "\n",
    "# tag::custom_tuner[]\n",
    "from ray.tune import logger, tuner\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "\n",
    "class NeptuneCallback(logger.LoggerCallback):\n",
    "    pass\n",
    "\n",
    "\n",
    "tuner = tuner.Tuner(\n",
    "    trainer,\n",
    "    run_config=RunConfig(callbacks=[NeptuneCallback(...)])\n",
    ")\n",
    "# end::custom_tuner[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
